{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Executive Summary:\n\n\n***\n\nThis is an analysis of the Portuguese Bank Telemarketing dataset. In the following sections I will be using Python to describe trends within the data and attempting to identify variables that have influence on outcomes (the 'y' variable) within the dataset. \n<br>\n\n**Process:**\n* I first identified and explored client-specific and selected campaign variables within the dataset. I selected the campaign variables based on the number of features belonging to each variable (which was few compared to some of the other campaign variables) in order to not overwhelm the step-wise selection function. \n* I created a logistic regression model based on the results of a step-wise feature selection function. \n    * I partitioned the data and plotted the AUC of the variables against the training and testing data, and then selected the best variables for the model. \n    * Ultimately, I selected 'contact_telephone', 'poutcome_success', 'job_blue-collar', 'age_41-50', and 'age_31-40' for the final iteration of the model. \n    \n**Findings:**\n* The model has 89% overall accuracy, and showed strong accuracy in predicting unsuccessful outcomes (where the client responded \"no\" to a deposit subscription.\n* While the model was not as accurate in terms of predicting successful outcomes, this could be a helpful model to predict which groups will not be effective targets in future campaigns\n\n**Rationale:**\n* I chose logistic regression as the method for developing a model for this dataset based on the ease of interpretation of the model. I've worked through several iterations of the model, and ended up narrowing down the dataset into groups of variables. I was initially interested in the economic variables, but then decided to use client-specific variables, as that might be a better fit in terms of usability. I figured that the bank might be more interested in potential targeting opportunities that could be employed in future campaigns, and adjusted my question and model accordingly. \n\n\n# Section 1: Importing Packages and Dataset\n***","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n#additional packages\nimport seaborn as sns\nimport scikitplot as skplt\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-19T20:19:34.368173Z","iopub.execute_input":"2023-03-19T20:19:34.368664Z","iopub.status.idle":"2023-03-19T20:19:34.381828Z","shell.execute_reply.started":"2023-03-19T20:19:34.368623Z","shell.execute_reply":"2023-03-19T20:19:34.380394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 2: Inspecting the Initial Dataset\n***\n\n### 2.1: Inspecting the columns and shape of the dataset\n\nBefore any analysis, we must first inspect the data and make sure that it's complete.","metadata":{}},{"cell_type":"code","source":"#loading the csv file into the pandas dataframe\ndf = pd.read_csv('/kaggle/input/dataset-for-bta-419-2023/BTA_419_2023_Data.csv')\n\n#checking the count of non-null values and data types\nprint(df.info())\n\n#inspecting the shape of the dataframe\nprint(\"The shape of the dataset is: \",df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:34.397481Z","iopub.execute_input":"2023-03-19T20:19:34.397985Z","iopub.status.idle":"2023-03-19T20:19:34.520709Z","shell.execute_reply.started":"2023-03-19T20:19:34.397939Z","shell.execute_reply":"2023-03-19T20:19:34.519631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All columns have the same counts of non-null values. The datatypes range from float64, int64, and objects. The shape of the dataset is 41,188 instances and 21 columns.","metadata":{}},{"cell_type":"code","source":"#checking for null values\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:34.522691Z","iopub.execute_input":"2023-03-19T20:19:34.523037Z","iopub.status.idle":"2023-03-19T20:19:34.549822Z","shell.execute_reply.started":"2023-03-19T20:19:34.523000Z","shell.execute_reply":"2023-03-19T20:19:34.548730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"None of the columns show any null values. ","metadata":{}},{"cell_type":"code","source":"#looking at the variables and head of the dataset\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:34.718016Z","iopub.execute_input":"2023-03-19T20:19:34.718485Z","iopub.status.idle":"2023-03-19T20:19:34.746992Z","shell.execute_reply.started":"2023-03-19T20:19:34.718445Z","shell.execute_reply":"2023-03-19T20:19:34.745311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2: Variables definition\nThe variable of interest is located within the 'y' column of the dataset, where a value of 'yes' indicates that the client subscribed to a term deposit.\nThere's a mix of categorical and quantitative variables in the dataset. A detailed breakdown of the variables is as follows:\n#### i. Input variables:\n##### <u>Bank client data:</u>\n1. age (numeric)\n2. job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n3. marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n4. education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n5. default: has credit in default? (categorical: 'no','yes','unknown')\n6. housing: has housing loan? (categorical: 'no','yes','unknown')\n7. loan: has personal loan? (categorical: 'no','yes','unknown')\n\n##### <u>Related with the last contact of the current campaign:</u>\n8. contact: contact communication type (categorical: 'cellular','telephone')\n9. month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n10. day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n11. duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n##### <u>Other attributes:</u>\n12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n14. previous: number of contacts performed before this campaign and for this client (numeric)\n15. poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n##### <u>Social and economic context attributes</u>\n16. emp.var.rate: employment variation rate - quarterly indicator (numeric)\n17. cons.price.idx: consumer price index - monthly indicator (numeric)\n18. cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n19. euribor3m: euribor 3 month rate - daily indicator (numeric)\n20. nr.employed: number of employees - quarterly indicator (numeric)\n\n#### ii. Output variable (desired target):\n21 - y - has the client subscribed a term deposit? (binary: 'yes','no')","metadata":{"execution":{"iopub.status.busy":"2023-03-17T20:20:10.142133Z","iopub.execute_input":"2023-03-17T20:20:10.142441Z","iopub.status.idle":"2023-03-17T20:20:10.158599Z","shell.execute_reply.started":"2023-03-17T20:20:10.142414Z","shell.execute_reply":"2023-03-17T20:20:10.155210Z"}}},{"cell_type":"markdown","source":"# Section 3: Business Question Definition + Exploring the Dataset\n***\n\n### 3.1: Defining the Business Question\n    \n   The Portugeuse bank would likely be interested in finding out which variables had a predictive nature towards the outcome variable, 'y'. A question related to this dataset could be: ***which demographic or campaign variables are likely to influence an outcome?*** This question could be relevant to the bank's future marketing efforts, and could result in savings or increased revenues for further campaigns. The demographic or campaign variables are ones that are likely to be known by the bank before the telemarketing campaign is being conducted, and thus could be of interest when developing a model for bank.\n   \n### 3.2: Exploring the Dataset\n\nIn order to develop an answer to the business question, we must first identify variables that might be candidates for a model. For this analysis, candidate variables for the model are ones that could be interpreted as having some sort of influence on the distribution of the outcome variable, 'y'.\n\n***\n#### i. The outcome variable 'y'\nThe first variable to look at is the outcome variable, 'y'. \n\n","metadata":{}},{"cell_type":"code","source":"#visualizing the 'y' variable outcomes\nsns.countplot(x='y',\n             data=df)\nplt.title(\"Y distribution\")\nplt.show()\nprint(df['y'].value_counts())\n\n#calculate the rate of subscription to term deposits\nsuccess_rate = round(sum(df['y'] == 'yes')/len(df['y'])*100,2)\nprint(\"The success rate is: \",success_rate, \"%\")\nfailure_rate = round(sum(df['y'] == 'no')/len(df['y'])*100,2)\nprint(\"The failure rate is: \", failure_rate, \"%\")","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:34.750602Z","iopub.execute_input":"2023-03-19T20:19:34.751042Z","iopub.status.idle":"2023-03-19T20:19:34.927905Z","shell.execute_reply.started":"2023-03-19T20:19:34.750977Z","shell.execute_reply":"2023-03-19T20:19:34.926856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The rate of successful outcomes for the entire dataset is 11.27% and the rate of failed outcomes is 88.73%. \n***\n\n#### ii. Client specific variables\nNext, we're looking at the client-specific variables, including: age, job, education, marital, loan, default, and housing.","metadata":{}},{"cell_type":"code","source":"#looking at the distribution of the demographic variables\nfig, axes = plt.subplots(1,2, figsize=(10,5))\nfig.suptitle(\"Age variable\")\nsns.violinplot(y='age',\n            x='y',\n           data=df,\n           ax=axes[0])\naxes[0].set_title(\"Age\")\nsns.histplot(x='age',\n            hue='y',\n            data=df,\n            ax=axes[1])\naxes[1].set_title(\"Age and y distribution\")\nplt.tight_layout()\nplt.show()\n\n#looking at the mean age of outcomes\nprint(\"Summary of mean ages by outcome: \")\nprint(df.groupby('y').age.mean())","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:34.929615Z","iopub.execute_input":"2023-03-19T20:19:34.929895Z","iopub.status.idle":"2023-03-19T20:19:35.805136Z","shell.execute_reply.started":"2023-03-19T20:19:34.929868Z","shell.execute_reply":"2023-03-19T20:19:35.803795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n* The average age of successful outcomes is a little bit higher than failed outcomes\n* Successful outcomes tended to be distributed more towards the higher end of the age range, where failed outcomes were grouped closer to the lower end\n* The age variable could be a candidate variable for the logistic regression model","metadata":{}},{"cell_type":"code","source":"#visualizing job and education\nfig, axes = plt.subplots(1,2, figsize=(10,5))\nfig.suptitle(\"Demographic variables\")\nsns.countplot(x='job',\n            data=df,\n            hue='y',\n            ax=axes[0])\naxes[0].tick_params(labelrotation=90)\naxes[0].set_title(\"Job\")\nsns.countplot(x='education',\n             data=df,\n              hue='y',\n             ax=axes[1])\naxes[1].tick_params(labelrotation=90)\naxes[1].set_title(\"Education\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:35.806525Z","iopub.execute_input":"2023-03-19T20:19:35.806858Z","iopub.status.idle":"2023-03-19T20:19:36.346212Z","shell.execute_reply.started":"2023-03-19T20:19:35.806815Z","shell.execute_reply":"2023-03-19T20:19:36.345231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting the ratios of outcomes\nprint(\"Job summary by y:\")\nprint(df.groupby('job').y.value_counts(normalize=True))","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:36.348325Z","iopub.execute_input":"2023-03-19T20:19:36.348608Z","iopub.status.idle":"2023-03-19T20:19:36.368716Z","shell.execute_reply.started":"2023-03-19T20:19:36.348581Z","shell.execute_reply":"2023-03-19T20:19:36.367451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting the ratios of outcomes\nprint(\"Education summary by y:\")\nprint(df.groupby('education').y.value_counts(normalize=True))","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:36.370288Z","iopub.execute_input":"2023-03-19T20:19:36.370821Z","iopub.status.idle":"2023-03-19T20:19:36.391038Z","shell.execute_reply.started":"2023-03-19T20:19:36.370792Z","shell.execute_reply":"2023-03-19T20:19:36.389330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n* The most common job types are admin and blue-collar, and the least common are unknown and housemaids; the job types with the highest rates of subscription are students and retirees, with 31% and 22% relating to successful outcomes, respectively\n* The most common education type is university degree and the least common is illiterate; the education type with the highest ratio of successful outcome is illiterate with 22%\n* Both job and education could be good candidate variables for the model","metadata":{}},{"cell_type":"code","source":"#visualizing the variable\nsns.countplot(x='marital',\n             data=df,\n              hue='y')\nplt.title(\"Marital\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:36.393536Z","iopub.execute_input":"2023-03-19T20:19:36.394005Z","iopub.status.idle":"2023-03-19T20:19:36.591918Z","shell.execute_reply.started":"2023-03-19T20:19:36.393952Z","shell.execute_reply":"2023-03-19T20:19:36.590575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#showing the percentages of outcomes by marital variable\nprint(\"Marital summary by y: \")\nprint(df.groupby('marital').y.value_counts(normalize=True))","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:36.593345Z","iopub.execute_input":"2023-03-19T20:19:36.594073Z","iopub.status.idle":"2023-03-19T20:19:36.610207Z","shell.execute_reply.started":"2023-03-19T20:19:36.594038Z","shell.execute_reply":"2023-03-19T20:19:36.609510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n* The most common marital type is married, the least common is unknown; the highest rate of successful outcome is unknown with 15% and then single with 14%\n* This could be a good candidate variable due to the variations of successful outcome rates\n","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,3, figsize=(10,5))\nfig.suptitle(\"Loan variables\")\nsns.countplot(x='default',\n           data=df,\n            hue='y',\n           ax=axes[0])\naxes[0].set_title(\"Default\")\nsns.countplot(x='housing',\n            data=df,\n            hue='y',\n            ax=axes[1])\naxes[1].set_title(\"Housing\")\nsns.countplot(x='loan',\n            data=df,\n            hue='y',\n            ax=axes[2])\naxes[2].set_title(\"Loan\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:36.611389Z","iopub.execute_input":"2023-03-19T20:19:36.612440Z","iopub.status.idle":"2023-03-19T20:19:37.145784Z","shell.execute_reply.started":"2023-03-19T20:19:36.612362Z","shell.execute_reply":"2023-03-19T20:19:37.143367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#showing the percentages of outcomes by variable\nprint(\"Default summary by y: \")\nprint(df.groupby('default').y.value_counts(normalize=True))","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.147927Z","iopub.execute_input":"2023-03-19T20:19:37.148377Z","iopub.status.idle":"2023-03-19T20:19:37.166909Z","shell.execute_reply.started":"2023-03-19T20:19:37.148334Z","shell.execute_reply":"2023-03-19T20:19:37.165427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#showing the percentages of outcomes by variable\nprint(\"Housing summary by y: \")\nprint(df.groupby('housing').y.value_counts(normalize=True))","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.170651Z","iopub.execute_input":"2023-03-19T20:19:37.171007Z","iopub.status.idle":"2023-03-19T20:19:37.191222Z","shell.execute_reply.started":"2023-03-19T20:19:37.170976Z","shell.execute_reply":"2023-03-19T20:19:37.189440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#showing the percentages of outcomes by variable\nprint(\"Loan summary by y: \")\nprint(df.groupby('loan').y.value_counts(normalize=True))","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.193100Z","iopub.execute_input":"2023-03-19T20:19:37.193971Z","iopub.status.idle":"2023-03-19T20:19:37.212455Z","shell.execute_reply.started":"2023-03-19T20:19:37.193925Z","shell.execute_reply":"2023-03-19T20:19:37.210523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n* The distribution of the outcome variable for all three of the loan variables follows the overall distribution of the 'y' variable for the dataset\n* The three loan variables are not candidate variables for the model\n\n***\n#### iii. Campaign variables\nNext, we'll look at the campaign variables, including: contact and poutcome. (Note: for the sake of the model, we will only look at two of the campaign variables.)\n","metadata":{}},{"cell_type":"code","source":"#visualizing contact\nfig, axes = plt.subplots(1,2, figsize=(10,5))\nfig.suptitle(\"Campaign variables\")\nsns.countplot(x='contact',\n           data=df,\n            hue='y',\n             ax=axes[0])\naxes[0].set_title(\"Contact\")\nsns.countplot(x='poutcome',\n            hue='y',\n           data=df,\n             ax=axes[1])\naxes[1].set_title(\"Poutcome\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.214436Z","iopub.execute_input":"2023-03-19T20:19:37.214795Z","iopub.status.idle":"2023-03-19T20:19:37.555501Z","shell.execute_reply.started":"2023-03-19T20:19:37.214753Z","shell.execute_reply":"2023-03-19T20:19:37.553865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#showing the percentages of outcomes by variable\nprint(\"Contact summary by y: \")\nprint(df.groupby('contact').y.value_counts(normalize=True))","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.557788Z","iopub.execute_input":"2023-03-19T20:19:37.558349Z","iopub.status.idle":"2023-03-19T20:19:37.578169Z","shell.execute_reply.started":"2023-03-19T20:19:37.558298Z","shell.execute_reply":"2023-03-19T20:19:37.576242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Summary of poutcome by outcome: \")\nprint(df.groupby('poutcome').y.value_counts(normalize=True))","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.582548Z","iopub.execute_input":"2023-03-19T20:19:37.583000Z","iopub.status.idle":"2023-03-19T20:19:37.600076Z","shell.execute_reply.started":"2023-03-19T20:19:37.582962Z","shell.execute_reply":"2023-03-19T20:19:37.598437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n* The most common method of contact is via cellular\n* The distribution of outcomes for the contact variable is outside the average distribution of the dataset, with 14% of clients reached via cellular related to a successful outcome compared to only 5% of clients reached via telephone\n* The distribution of outcomes for the poutcome variable is more varied than the average for the dataset\n* Both contact and poutcome are good candidate variables for the model\n\n***\n### 3.3 Identifying the candidate variables\nAfter the exploratory analysis, I've narrowed down the candidate variables to: age, job, education, marital, contact, and poutcome. Next, I will prepare the dataset for analysis with the logistic regression model.","metadata":{}},{"cell_type":"markdown","source":"# Section 4: Manipulating the dataset to prepare for analyses\n***\n\n### 4.1: Changing column datatypes for analysis\nFirst, I need to change the values in columns 'y' and 'age' to binary values to be used in the logistic regression model. ","metadata":{}},{"cell_type":"code","source":"#changing the target column value to 0,1\ndf['y'] = df['y'].replace(to_replace = ['yes', 'no'], value=[1,0])\ndf['y'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.601539Z","iopub.execute_input":"2023-03-19T20:19:37.601898Z","iopub.status.idle":"2023-03-19T20:19:37.627501Z","shell.execute_reply.started":"2023-03-19T20:19:37.601860Z","shell.execute_reply":"2023-03-19T20:19:37.626146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating age bins for the age variable\ndf['age']=pd.cut(x=df['age'], bins=[0,20,30,40,50,60,70,99],labels=['0-20','21-30','31-40','41-50','51-60','61-70','71-99'])\ndf['age'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.629023Z","iopub.execute_input":"2023-03-19T20:19:37.629507Z","iopub.status.idle":"2023-03-19T20:19:37.644675Z","shell.execute_reply.started":"2023-03-19T20:19:37.629461Z","shell.execute_reply":"2023-03-19T20:19:37.643271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2: Creating dummy columns\nNext, I need to create dummy columns with binary values for the categorical candidate variables.","metadata":{}},{"cell_type":"code","source":"#creating dummy columns for all categorical candidate variables\ncat_cols= ['age','job','education','marital', 'contact','poutcome']\ndummies = pd.get_dummies(df, prefix=cat_cols, columns=cat_cols)\nprint(dummies.columns)\ndummies.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.646621Z","iopub.execute_input":"2023-03-19T20:19:37.647043Z","iopub.status.idle":"2023-03-19T20:19:37.690077Z","shell.execute_reply.started":"2023-03-19T20:19:37.647006Z","shell.execute_reply":"2023-03-19T20:19:37.688768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dropping non-candidate columns\ndf = dummies.drop(dummies.columns[:14], axis=1)\nprint(df.columns)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.691343Z","iopub.execute_input":"2023-03-19T20:19:37.691725Z","iopub.status.idle":"2023-03-19T20:19:37.706826Z","shell.execute_reply.started":"2023-03-19T20:19:37.691688Z","shell.execute_reply":"2023-03-19T20:19:37.705393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 5: Developing a Model using Logistic Regression\n***\n\nAfter the inspection, initial analysis and manipulation of the dataset, I can now begin creating the logistic regression model using the candidate variables identified previously. \n\n\n### 5.1 Split the dataset into a training and testing sample\nFirst, I'll begin with splitting the dataset into a training and testing sample. I'll be using a test size of 30% with a random state of 16.","metadata":{}},{"cell_type":"code","source":"#splitting the dataset into train and test sets\nX = df.drop('y', axis=1)\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=16)\ntrain = pd.concat([X_train, y_train], axis=1)\ntest = pd.concat([X_test, y_test], axis=1)\nprint(\"The shape of the training dataset is: \", train.shape)\nprint(\"The shape of the testing dataset is: \", test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.708747Z","iopub.execute_input":"2023-03-19T20:19:37.709141Z","iopub.status.idle":"2023-03-19T20:19:37.726777Z","shell.execute_reply.started":"2023-03-19T20:19:37.709091Z","shell.execute_reply":"2023-03-19T20:19:37.725174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Defining base functions for variable selection\nNext, I'll be defining the functions that will be used to select the variables for the logistic regression function. I'll be using a stepwise selection process. The functions I'll be implementing are auc(), which will evaluate the AUC of each variable, and next_best(), which will select the next best variable based on the AUC calculation.","metadata":{}},{"cell_type":"code","source":"#creating a function to evaluate the auc\ndef auc(variables, target, basetable):\n    X = basetable[variables]\n    y = basetable[target]\n    logreg = linear_model.LogisticRegression()\n    logreg.fit(X,y.values.ravel())\n    predictions = logreg.predict_proba(X)[:,1]\n    auc=roc_auc_score(y, predictions)\n    return(auc)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.727955Z","iopub.execute_input":"2023-03-19T20:19:37.728406Z","iopub.status.idle":"2023-03-19T20:19:37.737450Z","shell.execute_reply.started":"2023-03-19T20:19:37.728359Z","shell.execute_reply":"2023-03-19T20:19:37.735382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating a function to evaluate the next best variable based on auc\ndef next_best(current_variables, candidate_variables, target, basetable):\n    best_auc = -1\n    best_variable = None\n    for v in candidate_variables:\n        auc_v = auc(current_variables + [v], target, basetable)\n        if auc_v >= best_auc:\n            best_auc=auc_v\n            best_variable=v\n    return best_variable","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.739519Z","iopub.execute_input":"2023-03-19T20:19:37.740236Z","iopub.status.idle":"2023-03-19T20:19:37.755925Z","shell.execute_reply.started":"2023-03-19T20:19:37.740196Z","shell.execute_reply":"2023-03-19T20:19:37.754480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 Entering the candidate variables into the stepwise selection functions\nNow, I'll use the previously defined functions to evaluate the candidate variables through a stepwise selection process. The output of the for loop will be a list called 'current_variables' that will contain the variables selected by the process.","metadata":{}},{"cell_type":"code","source":"#initializing the candidate_variables and current_variables\ncandidate_variables = ['age_0-20', 'age_21-30', 'age_31-40', 'age_41-50', 'age_51-60', 'age_61-70', 'age_71-99', 'job_admin.', 'job_blue-collar', 'job_entrepreneur', 'job_housemaid', 'job_management', 'job_retired', 'job_self-employed', 'job_services', 'job_student', 'job_technician', 'job_unemployed', 'job_unknown', 'education_basic.4y', 'education_basic.6y', 'education_basic.9y', 'education_high.school', 'education_illiterate', 'education_professional.course', 'education_university.degree', 'education_unknown', 'marital_divorced', 'marital_married', 'marital_single', 'marital_unknown', 'contact_cellular', 'contact_telephone', 'poutcome_failure', 'poutcome_nonexistent', 'poutcome_success']\ncurrent_variables = []\n\n#looping the variables through the functions in stepwise selection\nnumber_iterations = 10\nfor i in range(0, number_iterations):\n    next_variable = next_best(current_variables, candidate_variables, ['y'], df)\n    current_variables = current_variables + [next_variable]\n    candidate_variables.remove(next_variable)\n    print(\"Variable added in step \" + str(i+1)  + \" is \" + next_variable)\n\nprint('\\n', current_variables)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:37.772606Z","iopub.execute_input":"2023-03-19T20:19:37.773090Z","iopub.status.idle":"2023-03-19T20:19:56.951822Z","shell.execute_reply.started":"2023-03-19T20:19:37.773046Z","shell.execute_reply":"2023-03-19T20:19:56.951006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After entering the candidate variables into the stepwise selection loop, we can see the variables that have been selected based on their AUC calculations. \n\n### 5.4 Defining functions for visualization of the AUC curve\nNow that the variables have been selected, I'll create a function to visualize the AUC scores on the train and test samples and thus determine the cut-off point. I'll first create a function auc_train_test() that will create the train and testing lists of variables and the logistic regression model object. Then I'll fit the model based on the training values, and then calculate the predictions and AUC values on both the training and testing data.","metadata":{}},{"cell_type":"code","source":"#creating a function to create the training/testing variables\ndef auc_train_test(variables, target, train, test):\n    X_train = train[variables]\n    X_test = test[variables]\n    y_train = train[target]\n    y_test = test[target]\n    logreg = linear_model.LogisticRegression()\n    \n#fitting the model on train data\n    logreg.fit(X_train, y_train.values.ravel())\n    \n#calculating the predictions both on train and test data\n    predictions_train = logreg.predict_proba(X_train)[:,1]\n    predictions_test = logreg.predict_proba(X_test)[:,1]\n    \n#calculating the AUC both on train and test data\n    auc_train = roc_auc_score(y_train, predictions_train)\n    auc_test = roc_auc_score(y_test, predictions_test)\n    return(auc_train, auc_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:56.952990Z","iopub.execute_input":"2023-03-19T20:19:56.953467Z","iopub.status.idle":"2023-03-19T20:19:56.960045Z","shell.execute_reply.started":"2023-03-19T20:19:56.953437Z","shell.execute_reply":"2023-03-19T20:19:56.959315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.5 Entering the current variables into the AUC training and testing functions\nI'll next enter the current variables into the functions created to evaluate the AUC values partitioned by training and testing data.","metadata":{}},{"cell_type":"code","source":"# initializing lists of values\nauc_values_train = []\nauc_values_test = []\nvariables_evaluate = []\n\n#looping over the variables in current_variables\nfor v in current_variables:\n    variables_evaluate.append(v)\n    auc_train, auc_test = auc_train_test(variables_evaluate, ['y'], train, test)\n    auc_values_train.append(auc_train)\n    auc_values_test.append(auc_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:56.961371Z","iopub.execute_input":"2023-03-19T20:19:56.961904Z","iopub.status.idle":"2023-03-19T20:19:57.504304Z","shell.execute_reply.started":"2023-03-19T20:19:56.961868Z","shell.execute_reply":"2023-03-19T20:19:57.503431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.6 Plotting the AUC curve\nNext, I'll visualize the AUC values of the training and testing data to determine the cut-off point for the model.","metadata":{}},{"cell_type":"code","source":"#creating the AUC curve\nres = pd.DataFrame(dict(variables=current_variables, auc=auc_values_test))\nx = np.array(range(0,len(auc_values_train)))\ny_train = np.array(auc_values_train)\ny_test = np.array(auc_values_test)\nplt.xticks(x, current_variables, rotation = 90)\nplt.plot(x,y_train, label='Train')\nplt.plot(x,y_test, label='Test')\nplt.ylim((0.6, 0.8))\nplt.title(f'Best AUC = {round(res.auc.max(),3)}')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:19:57.505487Z","iopub.execute_input":"2023-03-19T20:19:57.506596Z","iopub.status.idle":"2023-03-19T20:19:57.695657Z","shell.execute_reply.started":"2023-03-19T20:19:57.506558Z","shell.execute_reply":"2023-03-19T20:19:57.694796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see in the plot, the AUC values of both the training and testing value seem to level off after the 'age_31-40' variable. This will be the cut-off point as to not add too much complexity to the model. The AUC values do increase after this point, but the increased value is marginal compared to the first five variables. The best variables for the model are thus: 'contact_telephone', 'poutcome_success', 'job_blue-collar', 'age_41-50', and 'age_31-40'.\n\n## Section 6: Evaluating the model\n***\nNow that I've created the model and identified the best variables according the their AUC values, we can now evaluate its performance.\n\n### 6.1 Plotting the cumulative gains curve\nFirst, I will enter the list of best variables determined by the AUC curve in the previous section and initialize the training and testing sets with these variables. Then I will create and fit model based on these sets, and finally plot the cumulative gains curve.","metadata":{}},{"cell_type":"code","source":"# initializing the best variables\nbest_vars=['contact_telephone','poutcome_success', 'job_blue-collar', 'age_41-50','age_31-40']\n\n#initializing train and test sets with best variables\nX_train = train[best_vars]\nX_test = test[best_vars]\ny_train = train['y']\ny_test = test['y']\n\n#creating logistic regression object\nlogreg = linear_model.LogisticRegression()\n    \n#fitting the model on the training data\nlogreg.fit(X_train, y_train)\n    \n#running predictions on testing data\npredictions_test = logreg.predict_proba(X_test)\n\n#ploting the cumulative gains graph\nskplt.metrics.plot_cumulative_gain(y_test, predictions_test)\n\nax = plt.gca()\nline = ax.lines[0]\nxd = line.get_xdata()\nyd = line.get_ydata()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-19T20:35:39.917237Z","iopub.execute_input":"2023-03-19T20:35:39.917678Z","iopub.status.idle":"2023-03-19T20:35:40.125484Z","shell.execute_reply.started":"2023-03-19T20:35:39.917640Z","shell.execute_reply":"2023-03-19T20:35:40.123909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the model performs well at the start of the cumulative gains curve; where the top 5% of customers with the highest predicted probabilities contain about 20% 'yes' respondents. \n\n### 6.2 Creating the confusion matrix\nFirst, I'll create the confusion matrix with the predicted data and use it to evaluate the model.","metadata":{}},{"cell_type":"code","source":"#defining the y prediction data\ny_pred = logreg.predict(X_test)\n\n#creating the confusion matrix\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix","metadata":{"execution":{"iopub.status.busy":"2023-03-19T21:30:20.498525Z","iopub.execute_input":"2023-03-19T21:30:20.498955Z","iopub.status.idle":"2023-03-19T21:30:20.516220Z","shell.execute_reply.started":"2023-03-19T21:30:20.498918Z","shell.execute_reply":"2023-03-19T21:30:20.514716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we'll visualize the confusion matrix.","metadata":{}},{"cell_type":"code","source":"#plotting the confusion matrix\nclass_names=[0,1]\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-19T21:31:33.383574Z","iopub.execute_input":"2023-03-19T21:31:33.384057Z","iopub.status.idle":"2023-03-19T21:31:33.606671Z","shell.execute_reply.started":"2023-03-19T21:31:33.384014Z","shell.execute_reply":"2023-03-19T21:31:33.605246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, as we can see, 10,786 + 257 are actual predictions and 1,166 + 148 are incorrect predictions. This seems to suggest that the model performs well (at least at predicting unsuccessful outcomes. \n\n### 6.3 Evaluating the model using classification_report\nNow we'll evaluate the accuracy, precision, and recall of the model.","metadata":{}},{"cell_type":"code","source":"#running the classification report on the model\ntarget_names = ['unsuccessful outcome', 'successful outcome']\nprint(classification_report(y_test, y_pred, target_names=target_names))","metadata":{"execution":{"iopub.status.busy":"2023-03-19T21:31:37.576177Z","iopub.execute_input":"2023-03-19T21:31:37.576663Z","iopub.status.idle":"2023-03-19T21:31:37.598379Z","shell.execute_reply.started":"2023-03-19T21:31:37.576613Z","shell.execute_reply":"2023-03-19T21:31:37.596945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the model has a classification rate of 89%. The model is 90% accurate when predicting unsuccessful outcomes, and 63% accurate when predicting successful outcomes. The model can detect unsuccessful outcomes 99% of the time, and successful outcomes 18% of the time. ","metadata":{}}]}